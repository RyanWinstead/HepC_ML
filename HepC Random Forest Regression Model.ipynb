{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HepC Random Forest Regression Model.ipynb","provenance":[{"file_id":"11mT7Z2at06HAjxdTYVhxmnRy4x5_e6Vp","timestamp":1607918269498},{"file_id":"1sCUMOtkThiFqdn525nopKaNYlontv-4D","timestamp":1607492004360},{"file_id":"https://github.com/Lore8614/Lore8614.github.io/blob/master/Big_Dreams.ipynb","timestamp":1607300047832}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"pXTDJN0Y83cA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613590874570,"user_tz":480,"elapsed":5260,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"4a453c88-433f-4947-9f58-959b949b4e84"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt \n","from sklearn.tree import export_graphviz\n","import pydot\n","from sklearn.tree import export_graphviz\n","import pydot\n","%matplotlib inline\n","from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","from sklearn import linear_model \n","import statsmodels.api as sm\n","from sklearn.metrics import r2_score\n","import math as m\n","from pandas import DataFrame\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YsMJXQv6BGf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613590901447,"user_tz":480,"elapsed":32037,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"d3daefd7-03d5-4b8f-bbe0-a7a1a20367b6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQLQMsPjEJCs","executionInfo":{"status":"ok","timestamp":1613590902262,"user_tz":480,"elapsed":27282,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"fc1308d9-1d28-41e2-834a-b37f9690f3c9"},"source":["%pwd\n","%ls '/content/drive/My Drive/Machine Learning Final'"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" allfeatures_final.csv\n"," allfeatures_final.gsheet\n"," allfeatures_num_OH.csv\n","'Before_HepC Random Forest Regression Model_LBR.ipynb'\n","'Big Dreams.ipynb'\n"," \u001b[0m\u001b[01;34mData\u001b[0m/\n"," DATACHALLENGE.ipynb\n"," DecisionTreeCode.ipynb\n"," Geller.mutation.rates_update.csv\n"," GrabFeaturesCode.py\n"," H77_metadata.csv\n"," H77_metadata.gsheet\n"," HCV1a_TsMutFreq_195.csv\n"," HCV1a_TsMutFreq_195.gsheet\n","'HepC Random Forest Regression Model.ipynb'\n","'Histogram of Natural log of costs.png'\n"," one_hot_features.py\n"," RNAStructure_Conserved.csv\n"," RNAStructure_Conserved.gsheet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WA_wm7xxDmcQ","executionInfo":{"status":"ok","timestamp":1613590934605,"user_tz":480,"elapsed":703,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}}},"source":["#pos_muts = pd.read_csv('/content/drive/My Drive/Machine Learning Final/H77_metadata.csv')\n","#freqs = pd.read_csv('/content/drive/My Drive/Machine Learning Final/HCV1a_TsMutFreq_195.csv')\n","#mut_rate = pd.read_csv('/content/drive/My Drive/Machine Learning Final/Geller.mutation.rates_update.csv')\n","#allfeatures_num_OH_OG = pd.read_csv('/content/drive/My Drive/Machine Learning Final/allfeatures_num_OH.csv') \n","#logCosts = pd.read_csv('/content/drive/My Drive/Machine Learning Final/logCosts.csv') \n","#allfeatures_num_OH = pd.concat([allfeatures_num_OH_OG,logCosts],axis=1,sort=False)\n","\n","#allfeatures_num_OH.head()\n","\n","features = pd.read_csv('/content/drive/My Drive/Machine Learning Final/allfeatures_final.csv') \n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"PdjEYJrFISfb","executionInfo":{"status":"ok","timestamp":1613590938039,"user_tz":480,"elapsed":896,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"0873b5f2-d9d8-4359-c130-f94fec846b9b"},"source":["features.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>makesCpG</th>\n","      <th>bigAAChange</th>\n","      <th>Avg_Mutation_Freq</th>\n","      <th>Mutation_Rate</th>\n","      <th>Cost</th>\n","      <th>Positive AA</th>\n","      <th>Negative AA</th>\n","      <th>Hydrophobic AA</th>\n","      <th>Polar AA</th>\n","      <th>Nonpolar AA</th>\n","      <th>5' UTR</th>\n","      <th>Core</th>\n","      <th>E1</th>\n","      <th>E2</th>\n","      <th>HVR1</th>\n","      <th>NS1</th>\n","      <th>NS2</th>\n","      <th>NS3</th>\n","      <th>NS4A</th>\n","      <th>NS4B</th>\n","      <th>NS5A</th>\n","      <th>NS5B</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>264</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.183585</td>\n","      <td>0.000011</td>\n","      <td>0.000062</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>265</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.001653</td>\n","      <td>0.000011</td>\n","      <td>0.006840</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>266</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000053</td>\n","      <td>0.000003</td>\n","      <td>0.059900</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>267</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000089</td>\n","      <td>0.000003</td>\n","      <td>0.035500</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>268</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000069</td>\n","      <td>0.000003</td>\n","      <td>0.045800</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pos  makesCpG  bigAAChange  Avg_Mutation_Freq  ...  NS4A  NS4B  NS5A  NS5B\n","0  264         0            0           0.183585  ...     0     0     0     0\n","1  265         1            1           0.001653  ...     0     0     0     0\n","2  266         0            0           0.000053  ...     0     0     0     0\n","3  267         0            1           0.000089  ...     0     0     0     0\n","4  268         0            1           0.000069  ...     0     0     0     0\n","\n","[5 rows x 23 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"0KzkyurmzfC7"},"source":["# Labels are the values we want to predict\r\n","labels=[]\r\n","for num in features['Cost']:\r\n","    ln = m.log(num)\r\n","    labels.append(ln)\r\n","labels_df = DataFrame(labels, columns=['ln_Cost'])\r\n","\r\n","labels = np.array(labels_df['ln_Cost'])\r\n","\r\n","\r\n","# Remove the labels from the features\r\n","# axis 1 refers to the columns\r\n","features= features.drop('Cost', axis = 1)\r\n","cpg = features['makesCpG']\r\n","otherfeats = features.iloc[:,6:]\r\n","feats = pd.concat([cpg,otherfeats],axis=1,sort=False)\r\n","# Saving feature names for later use\r\n","feature_list = list(feats.columns)\r\n","# Convert to numpy array\r\n","features = np.array(feats)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vi0oxvXAc2k4"},"source":["#TRAINING AND TESTING SETS\r\n","# Using Skicit-learn to split data into training and testing sets\r\n","#from sklearn.model_selection import train_test_split\r\n","# Split the data into training and testing sets\r\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwxFdiZXL1ay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613156053176,"user_tz":480,"elapsed":34770,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"7e835b60-e088-4221-c1b5-5d341377ecad"},"source":["#look at the shape of all the data to make sure we did everything correct\r\n","print('Training Features Shape:', train_features.shape)\r\n","print('Training Labels Shape:', train_labels.shape)\r\n","print('Testing Features Shape:', test_features.shape)\r\n","print('Testing Labels Shape:', test_labels.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Features Shape: (6280, 17)\n","Training Labels Shape: (6280,)\n","Testing Features Shape: (2094, 17)\n","Testing Labels Shape: (2094,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7U34EjXw2Fhj"},"source":["It looks as if everything is in order! \r\n","Just to recap, to get the data into a form acceptable for machine learning we:\r\n","\r\n","\r\n","1.   One-hot encoded categorical variables\r\n","2.   Split data into features and labels\r\n","3.   Converted to arrays\r\n","4.   Split data into training and testing sets\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"ieugc1do89sE"},"source":["**Establish Baseline**\r\n","Before we can make and evaluate predictions, we need to establish a baseline, a sensible measure that we hope to beat with our model. \r\n","If our model cannot improve upon the baseline, then it will be a failure and we should try a different model or admit that machine learning is not right for our problem. \r\n","The baseline prediction for our case can be the historical HIGHEST COST averages. In other words, our baseline is the error we would get if we simply predicted the \r\n","average HIGHEST COST for all FEATURES."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"soSzmR8gsE9a","executionInfo":{"status":"error","timestamp":1613156053620,"user_tz":480,"elapsed":35195,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"72b1bc7e-15b0-4be7-c807-d7996d92e68e"},"source":["#Did not run this section of code, I think we need to use average cost values for each feature that Lorena calculated -Madu\r\n","# The baseline predictions are the historical averages\r\n","baseline_preds = test_features[:, feature_list.index('Avg_Mutation_Freq')]\r\n","# Baseline errors, and display average baseline error\r\n","baseline_errors = abs(baseline_preds - test_labels)\r\n","print('Average baseline error: ', round(np.mean(baseline_errors), 4))\r\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c1507b71c7c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Did not run this section of code, I think we need to use average cost values for each feature that Lorena calculated -Madu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The baseline predictions are the historical averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbaseline_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Avg_Mutation_Freq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Baseline errors, and display average baseline error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbaseline_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_preds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'Avg_Mutation_Freq' is not in list"]}]},{"cell_type":"markdown","metadata":{"id":"w3CQ3DZB44uL"},"source":["We now have our goal! If we can’t beat an average error of 0.0606, then we need to rethink our approach.\r\n"]},{"cell_type":"markdown","metadata":{"id":"UJBLOjM-8cGV"},"source":["**TRAIN MODEL**\r\n","\r\n","After all the work of data preparation, creating and training the model is pretty simple using Scikit-learn. We import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn’s name for training) the model on the training data. (Again setting the random state for reproducible results). This entire process is only 3 lines in scikit-learn!"]},{"cell_type":"code","metadata":{"id":"dHCORexdslet","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613156303653,"user_tz":480,"elapsed":6260,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"28e8bdae-4d73-41e7-c03f-f46d2dd1806b"},"source":["#TRAIN MODEL\r\n","#Don't really know what values like R^2 Training Score means\r\n","# Instantiate model with 1000 decision trees\r\n","rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\r\n","# Train the model on training data\r\n","rf.fit(train_features, train_labels);\r\n","print('R^2 Training Score: {:.2f} \\nR^2 Testing Score: {:.2f}'.format(rf.score(train_features, train_labels), \r\n","                                                                                             rf.score(test_features, test_labels)))\r\n","#print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Testing Score: {:.2f}'.format(rf.score(train_features, train_labels), \r\n","                                                                                             #rf.oob_score_,\r\n","                                                                                             #rf.score(test_features, test_labels)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["R^2 Training Score: 0.25 \n","R^2 Testing Score: 0.22\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_UlFNAdi5ugj"},"source":["**MAKE PREDICTIONS ON THE TEST SET**\r\n","\r\n","Our model has now been trained to learn the relationships between the features and the targets. The next step is figuring out how good the model is! To do this we make predictions on the test features (the model is never allowed to see the test answers). We then compare the predictions to the known answers. When performing regression, we need to make sure to use the absolute error because we expect some of our answers to be low and some to be high. We are interested in how far away our average prediction is from the actual value so we take the absolute value (as we also did when establishing the baseline)."]},{"cell_type":"markdown","metadata":{"id":"4ZM7v-N0VGAj"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GftO4c105_gh","executionInfo":{"status":"ok","timestamp":1613156306092,"user_tz":480,"elapsed":531,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"2df94d22-6dd1-45fe-cb46-2728401ea409"},"source":["# Use the forest's predict method on the test data\r\n","predictions = rf.predict(test_features)\r\n","#print(predictions)\r\n","# Calculate the absolute errors\r\n","errors = abs(predictions - test_labels)\r\n","print(errors)\r\n","# Print out the mean absolute error (mae)\r\n","print('Mean Absolute Error:', round(np.mean(errors), 4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.45424684 1.67481478 1.55715537 ... 1.57269057 2.11628589 0.59706956]\n","Mean Absolute Error: 1.3403\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyZJb5TrIZO6","executionInfo":{"status":"ok","timestamp":1613156307856,"user_tz":480,"elapsed":391,"user":{"displayName":"Madu Nzerem","photoUrl":"","userId":"03371296412911858816"}},"outputId":"acc3900d-2691-490d-d8e5-831f41319d45"},"source":["# Calculate mean absolute percentage error (MAPE)\r\n","mape = 100 * (errors / abs(test_labels))\r\n","# Calculate and display accuracy\r\n","accuracy = 100 - np.mean(mape)\r\n","print('Accuracy:', round(accuracy, 2), '%.')\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 81.19 %.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CBo8vmny9jKA"},"source":["Our average estimate is off by 0.0003. compare to average improvement over the baseline that is 0.0606. Having a 6% difference"]},{"cell_type":"markdown","metadata":{"id":"wKap-VG-BPHB"},"source":["**Interpret Model and Report Results**\r\n","\r\n","At this point, we know our model is good, but it’s pretty much a black box. We feed in some Numpy arrays for training, ask it to make a prediction, evaluate the predictions, and see that they are reasonable. The question is: how does this model arrive at the values? There are two approaches to get under the hood of the random forest: first, we can look at a single tree in the forest, and second, we can look at the feature importances of our explanatory variables."]},{"cell_type":"markdown","metadata":{"id":"DdwIPY-9Big0"},"source":["**Visualizing a Single Decision Tree**\r\n","\r\n","One of the coolest parts of the Random Forest implementation in Skicit-learn is we can actually examine any of the trees in the forest. We will select one tree, and save the whole tree as an image.\r\n","The following code takes one tree from the forest and saves it as an image."]},{"cell_type":"code","metadata":{"id":"9IRLB-Gu00YN"},"source":["# Import tools needed for visualization\n","#from sklearn.tree import export_graphviz\n","#import pydot\n","\n","# Pull out one tree from the forest\n","tree = rf.estimators_[5]\n","# Import tools needed for visualization\n","#from sklearn.tree import export_graphviz\n","#import pydot\n","\n","# Pull out one tree from the forest\n","tree = rf.estimators_[5]\n","\n","# Export the image to a dot file\n","export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n","\n","# Use dot file to create a graph\n","(graph, ) = pydot.graph_from_dot_file('tree.dot')\n","\n","# Write graph to a png file\n","graph.write_png('tree.png')\n","\n","# Display in python\n","#import matplotlib.pyplot as plt\n","plt.figure(figsize = (50, 50))\n","plt.imshow(plt.imread('tree.png'))\n","plt.axis('off');\n","plt.show();\n","\n","# Download tree\n","from google.colab import files\n","plt.savefig('tree.png')\n","files.download('tree.png') \n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pA5BxTDiFNci"},"source":["# Limit depth of tree to 3 levels\r\n","rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\r\n","rf_small.fit(train_features, train_labels)\r\n","\r\n","# Extract the small tree\r\n","tree_small = rf_small.estimators_[5]\r\n","\r\n","# Save the tree as a png image\r\n","export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\r\n","(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\r\n","graph.write_png('small_tree.png');\r\n","\r\n","# Display in python\r\n","#import matplotlib.pyplot as plt\r\n","plt.figure(figsize = (25, 25))\r\n","plt.imshow(plt.imread('small_tree.png'))\r\n","plt.axis('off');\r\n","plt.show();\r\n","\r\n","# Download tree\r\n","from google.colab import files\r\n","plt.savefig('small_tree.png')\r\n","files.download('small_tree.png') \r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBsVVQh7FkY2"},"source":["**Variable Importances**\r\n","\r\n","In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables. The importances returned in Skicit-learn represent how much including a particular variable improves the prediction. The actual calculation of the importance is beyond the scope of this post, but we can use the numbers to make relative comparisons between variables.\r\n","The code here takes advantage of a number of tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. It’s not that important to understand these at the moment, but if you want to become skilled at Python, these are tools you should have in your arsenal!"]},{"cell_type":"code","metadata":{"id":"c2vLDX2mJRyr"},"source":["# Get numerical feature importances\r\n","importances = list(rf.feature_importances_)\r\n","# List of tuples with variable and importance\r\n","feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\r\n","# Sort the feature importances by most important first\r\n","feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\r\n","# Print out the feature and importances \r\n","[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1k9CheUbJ0J-"},"source":["# Didn't run this section of code cus all of our vairables have some kind of importance in predicting cost -Madu\r\n","# New random forest with only the two most important variables\r\n","rf_most_important = RandomForestRegressor(n_estimators = 1000 , n_jobs = -1, oob_score = True, bootstrap= True, random_state = 42)\r\n","\r\n","# Extract the two most important features\r\n","important_indices = [feature_list.index('Avg_Mutation_Freq'), feature_list.index('pos')]\r\n","train_important = train_features[:, important_indices]\r\n","test_important = test_features[:, important_indices]\r\n","\r\n","# Train the random forest\r\n","rf_most_important.fit(train_important, train_labels)\r\n","print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Testing Score: {:.2f}'.format(rf_most_important.score(train_important, train_labels), \r\n","                                                                                             rf_most_important.oob_score_,\r\n","                                                                                             rf_most_important.score(test_important, test_labels)))\r\n","\r\n","# Make predictions and determine the error\r\n","predictions = rf_most_important.predict(test_important)\r\n","errors = abs(predictions - test_labels)\r\n","\r\n","# Display the performance metrics\r\n","print('Mean Absolute Error:', round(np.mean(errors), 4))\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zGyPZHeK3db"},"source":["**Visualizations**\r\n","\r\n","The first chart I’ll make is a simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables. Plotting in Python is kind of non-intuitive, and I end up looking up almost everything on Stack Overflow when I make graphs. Don’t worry if the code here doesn’t quite make sense, sometimes fully understanding the code isn’t necessary to get the end result you want!"]},{"cell_type":"code","metadata":{"id":"CqduUs7zK_5d"},"source":["# Import matplotlib for plotting and use magic command for Jupyter Notebooks\r\n","#import matplotlib.pyplot as plt\r\n","#%matplotlib inline\r\n","# Set the style\r\n","plt.style.use('fivethirtyeight')\r\n","\r\n","# list of x locations for plotting\r\n","x_values = list(range(len(importances)))\r\n","\r\n","# Make a bar chart\r\n","plt.bar(x_values, importances, orientation = 'vertical')\r\n","\r\n","# Tick labels for x axis\r\n","plt.xticks(x_values, feature_list, rotation='vertical')\r\n","\r\n","# Axis labels and title\r\n","plt.ylabel('Importance'); plt.xlabel('Features'); plt.title('Features Importances');"],"execution_count":null,"outputs":[]}]}